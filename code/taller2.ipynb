{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrios80/GroceryStoreDataset/blob/master/code/taller2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TALLER 2\n",
        "\n",
        "### Objetivo:\n",
        "aplicar técnicas de machine learning, las cuales permitan descubrir insights, sugerir accionables al negocio y calcular el valor ganado.\n",
        "\n",
        "### Contexto del negocio\n",
        "Apoyo a un Supermercado Inteligente\n",
        "\n",
        "### Mision\n",
        "Mediante el uso de modelos de Machine Learning, en conjunto con técnicas de preparación de datos, se espera que usted esté en capacidad de construir el modelo que identifique los productos, y argumente el valor que generará al supermercado los resultados que obtenga.\n"
      ],
      "metadata": {
        "id": "65L8-jED_dhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Entendimiento y preparación de los datos:\n",
        "\n",
        "Reporte de entendimiento de datos.\n",
        "\n",
        "*   Dimensiones del dataset\n",
        "*   Caracteristicas de las imagenes\n",
        "*   Indicadores importantes\n",
        "*   Intergracion de tecnicas de aumento de datos\n",
        "*   Determinacion de productos y que categorías a emplear\n"
      ],
      "metadata": {
        "id": "qlRdH4BDAivd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Dimensiones del dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "W28kD0VTzE9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata=pd.read_csv('https://raw.githubusercontent.com/amrios80/GroceryStoreDataset/refs/heads/master/compressed/metadata.csv')\n",
        "#metadata[\"mode\"].value_counts()\n",
        "#print(metadata[\"extension\"].value_counts())\n",
        "print(\"numero de imagenes \" + str(metadata[metadata[\"path\"].str.contains(\".jpg\")].shape[0]) )\n",
        "print(\"archivos por modo:\" + str( metadata[\"mode\"].value_counts() ))\n"
      ],
      "metadata": {
        "id": "qjDwXFACzfUM",
        "outputId": "f512f02f-b6a0-4691-fe0a-58dc6996f491",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numero de imagenes 5418\n",
            "archivos por modo:mode\n",
            "train    2639\n",
            "test     2484\n",
            "val       295\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Caracteristicas de las imagenes\n",
        "\n"
      ],
      "metadata": {
        "id": "wVy5G-4x0ipV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata[[\"size\",\"size_x\",\"size_y\",\"channels\"]].describe()"
      ],
      "metadata": {
        "id": "Hnkk2uh10pT5",
        "outputId": "5dd71cef-25c1-4c92-e557-f89d0b73cc75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               size       size_x       size_y  channels\n",
              "count   5418.000000  5418.000000  5418.000000    5418.0\n",
              "mean   22420.337025   348.792174   357.120709       3.0\n",
              "std     4131.906988     9.554136    31.224907       0.0\n",
              "min     9730.000000   348.000000   348.000000       3.0\n",
              "25%    19756.000000   348.000000   348.000000       3.0\n",
              "50%    22048.500000   348.000000   348.000000       3.0\n",
              "75%    24551.750000   348.000000   348.000000       3.0\n",
              "max    41049.000000   464.000000   464.000000       3.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-316ce998-3150-4eba-9970-12ad5c7e1546\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>size</th>\n",
              "      <th>size_x</th>\n",
              "      <th>size_y</th>\n",
              "      <th>channels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5418.000000</td>\n",
              "      <td>5418.000000</td>\n",
              "      <td>5418.000000</td>\n",
              "      <td>5418.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>22420.337025</td>\n",
              "      <td>348.792174</td>\n",
              "      <td>357.120709</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4131.906988</td>\n",
              "      <td>9.554136</td>\n",
              "      <td>31.224907</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>9730.000000</td>\n",
              "      <td>348.000000</td>\n",
              "      <td>348.000000</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>19756.000000</td>\n",
              "      <td>348.000000</td>\n",
              "      <td>348.000000</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>22048.500000</td>\n",
              "      <td>348.000000</td>\n",
              "      <td>348.000000</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>24551.750000</td>\n",
              "      <td>348.000000</td>\n",
              "      <td>348.000000</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>41049.000000</td>\n",
              "      <td>464.000000</td>\n",
              "      <td>464.000000</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-316ce998-3150-4eba-9970-12ad5c7e1546')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-316ce998-3150-4eba-9970-12ad5c7e1546 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-316ce998-3150-4eba-9970-12ad5c7e1546');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-23937409-65a3-4686-a6ae-c90c5cfcdb11\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-23937409-65a3-4686-a6ae-c90c5cfcdb11')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-23937409-65a3-4686-a6ae-c90c5cfcdb11 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"metadata[[\\\"size\\\",\\\"size_x\\\",\\\"size_y\\\",\\\"channels\\\"]]\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12124.122742146588,\n        \"min\": 4131.906988292787,\n        \"max\": 41049.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          22420.337024732373,\n          22048.5,\n          5418.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"size_x\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1808.4956055471546,\n        \"min\": 9.554136222418641,\n        \"max\": 5418.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          348.7921742340347,\n          464.0,\n          9.554136222418641\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"size_y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1806.4963584959467,\n        \"min\": 31.224907389989184,\n        \"max\": 5418.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          357.1207087486157,\n          464.0,\n          31.224907389989184\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"channels\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1914.6434207369864,\n        \"min\": 0.0,\n        \"max\": 5418.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5418.0,\n          3.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Indicadores"
      ],
      "metadata": {
        "id": "eb0CE3Op7-vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"cantidad de clases        :\"+str(metadata[\"class_id\"].nunique()))\n",
        "print(\"cantidad de coarse classes:\"+str(metadata[\"coarse_class_id\"].nunique()))\n",
        "print(\"Tamaño en disco           :\"+str(metadata[\"size\"].sum()/1000000 )+ \" MB \")\n",
        "print(\"numero de archivos        :\"+str(metadata.shape[0]))\n",
        "print(\"Size promedio             :\"+str(metadata[\"size\"].mean()/1000 )+ \" KB\")\n",
        "print(\"Categorias                : 3   (Fruit, Packages, Vegetables)\")"
      ],
      "metadata": {
        "id": "r3ZO-qFM76Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "XbG3ycV774EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Entendimiento y preparacion de los datos.\n",
        "#Dataset:\n",
        "\n",
        "#Imagenes: 348x348 96dpi 24 bit depth"
      ],
      "metadata": {
        "id": "3LjNxjPz74EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install scikit-learn pandas numpy opencv-python\n",
        "\n",
        "\n",
        "#training\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data_dir = '/kaggle/input/apples/golden-delicious/'\n",
        "labels_file = os.path.join(data_dir, 'classes.csv')\n",
        "label_file_columns=[\"class_name\",\"class_id\",\"coarse_class_name\",\"coarse_class_id\",\"icon_path\",\"product_description_path\"]\n",
        "\n",
        "# Read the labels\n",
        "labels_df = pd.read_csv(labels_file)\n",
        "labels_df.columns=label_file_columns\n",
        "#print(labels_df)\n",
        "\n",
        "def  load_file(data_dir='', mode=\"train\"):\n",
        "    train_file=f\"{data_dir}{mode}.txt\"\n",
        "    dftrain_file=pd.read_csv(train_file)\n",
        "    train_file_columns=[\"filename\",\"class_id\", \"coarse_class_id\"]\n",
        "    dftrain_file.columns=train_file_columns\n",
        "    return dftrain_file\n",
        "#print(dftrain_file)\n",
        "\n",
        "dftrain_file=load_file(data_dir=data_dir, mode=\"train\")\n",
        "\n",
        "print('rows:' + str(dftrain_file.shape[0]))\n",
        "\n",
        "\n",
        "# Function to load images into an array\n",
        "def load_images(data_dir, df_files, image_size=348 ):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for index, row in df_files.iterrows():\n",
        "        img_path = os.path.join(data_dir, row['filename'])\n",
        "        #print(img_path)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.resize(img, (image_size, image_size))  # Resize to a fixed size\n",
        "        images.append(img)\n",
        "        labels.append(row['class_id'])\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "\n",
        "def preprocess_dataset(data_dir='', mode=\"train\" ):\n",
        "    dftrain_file = load_file(data_dir, mode)\n",
        "    images, labels = load_images(data_dir, dftrain_file )\n",
        "    # Normalize the images\n",
        "    # This line normalizes the pixel values of the images by dividing them by 255.0. This scales the pixel values from the original range (typically 0-255 for RGB images) to the range 0-1, which is a common normalization technique for neural networks.\n",
        "    # images = images / 255.0\n",
        "    # Flatten the images for the classifier\n",
        "    #This reshapes the images from their original shape\n",
        "    # (likely (n_samples, height, width, channels)) to a flat 2D array.\n",
        "    # The new shape is (n_samples, height * width * channels), where:\n",
        "    # height * width * channels represents the total number of pixels in each image.\n",
        "    # n_samples is the number of images in the dataset.\n",
        "    n_samples, height, width, channels = images.shape\n",
        "    print('before shaping:')\n",
        "    print(images.shape)\n",
        "    print( n_samples, height, width, channels )\n",
        "    images = images.reshape((n_samples, height * width * channels))\n",
        "    print('after reshaping:')\n",
        "    print(images.shape)\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "def preprocess_dataset_no_flattening(data_dir='', mode=\"train\" ):\n",
        "    dftrain_file = load_file(data_dir, mode)\n",
        "    images, labels = load_images(data_dir, dftrain_file, image_size=128 )\n",
        "    # Normalize the images\n",
        "    # This line normalizes the pixel values of the images by dividing them by 255.0. This scales the pixel values from the original range (typically 0-255 for RGB images) to the range 0-1, which is a common normalization technique for neural networks.\n",
        "    images = images / 255.0\n",
        "    print(images.shape)\n",
        "    return images, labels\n",
        "\n",
        "X_train, y_train = preprocess_dataset( data_dir,\"train\" )\n",
        "print(X_train)\n",
        "print(X_train.shape)\n",
        "print(y_train)\n",
        "\n",
        "X_test, y_test = preprocess_dataset( data_dir,\"test\" )\n",
        "\n",
        "\n",
        "# Encode the labels if necessary\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# le = LabelEncoder()\n",
        "# labels = le.fit_transform(labels)\n",
        "\n",
        "# Split the data\n",
        "#X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.0, random_state=42)\n",
        "#X_train =images\n",
        "#y_train =labels\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "###############\n",
        "\n",
        "# Initialize the classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-28T23:33:10.628042Z",
          "iopub.execute_input": "2024-10-28T23:33:10.628839Z",
          "iopub.status.idle": "2024-10-28T23:33:19.416887Z",
          "shell.execute_reply.started": "2024-10-28T23:33:10.628792Z",
          "shell.execute_reply": "2024-10-28T23:33:19.415631Z"
        },
        "trusted": true,
        "id": "c_ukxBIa74EF",
        "outputId": "4e74497e-50dc-498d-9ebb-4742f3239ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "rows:277\nbefore shaping:\n(277, 348, 348, 3)\n277 348 348 3\nafter reshaping:\n(277, 363312)\n[[ 37  20 147 ...  29 126 160]\n [ 13  61  89 ...  79 107 142]\n [ 26 144 193 ...  37  82 115]\n ...\n [ 22  33  47 ...  43  68 102]\n [ 43  65 171 ... 104 181 243]\n [  0 114 117 ...  40  35 144]]\n(277, 363312)\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4\n 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\nbefore shaping:\n(275, 348, 348, 3)\n275 348 348 3\nafter reshaping:\n(275, 363312)\n(277, 363312)\nAccuracy: 59.27%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the classifier\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'XGBoost Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-28T23:23:12.572640Z",
          "iopub.execute_input": "2024-10-28T23:23:12.573092Z",
          "iopub.status.idle": "2024-10-28T23:28:14.238742Z",
          "shell.execute_reply.started": "2024-10-28T23:23:12.573048Z",
          "shell.execute_reply": "2024-10-28T23:28:14.237191Z"
        },
        "trusted": true,
        "id": "c02bLRql74EH",
        "outputId": "357a6b90-a186-4e7e-d05c-b5a4633ae463"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "\nKeyboardInterrupt\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "\n",
        "\n",
        "X_train, y_train = preprocess_dataset_no_flattening( data_dir,\"train\" )\n",
        "X_test, y_test = preprocess_dataset_no_flattening( data_dir,\"test\" )\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')  # Assuming 10 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'CNN Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-28T23:33:24.709776Z",
          "iopub.execute_input": "2024-10-28T23:33:24.710655Z",
          "iopub.status.idle": "2024-10-28T23:34:13.055423Z",
          "shell.execute_reply.started": "2024-10-28T23:33:24.710605Z",
          "shell.execute_reply": "2024-10-28T23:34:13.054214Z"
        },
        "trusted": true,
        "id": "Io8MqXDp74EI",
        "outputId": "c9391b28-545b-4b39-b03c-678f14f01656"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "(277, 128, 128, 3)\n(275, 128, 128, 3)\nEpoch 1/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 523ms/step - accuracy: 0.2620 - loss: 4.0262 - val_accuracy: 0.4436 - val_loss: 1.1451\nEpoch 2/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 471ms/step - accuracy: 0.5777 - loss: 0.9984 - val_accuracy: 0.5964 - val_loss: 0.8401\nEpoch 3/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 471ms/step - accuracy: 0.7931 - loss: 0.5782 - val_accuracy: 0.5309 - val_loss: 0.8602\nEpoch 4/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 455ms/step - accuracy: 0.9280 - loss: 0.3139 - val_accuracy: 0.6727 - val_loss: 0.6822\nEpoch 5/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 468ms/step - accuracy: 0.9908 - loss: 0.1264 - val_accuracy: 0.7273 - val_loss: 0.6469\nEpoch 6/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 466ms/step - accuracy: 1.0000 - loss: 0.0425 - val_accuracy: 0.6691 - val_loss: 0.7850\nEpoch 7/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 445ms/step - accuracy: 1.0000 - loss: 0.0164 - val_accuracy: 0.6509 - val_loss: 0.8393\nEpoch 8/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 453ms/step - accuracy: 1.0000 - loss: 0.0082 - val_accuracy: 0.6582 - val_loss: 0.8317\nEpoch 9/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 477ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.6873 - val_loss: 0.8328\nEpoch 10/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 464ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.6618 - val_loss: 0.8208\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.6684 - loss: 0.9236\nCNN Accuracy: 66.18%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize the classifier\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and accuracy\n",
        "best_params = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "print(f'Best Parameters: {best_params}')\n",
        "print(f'Best Accuracy: {best_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-28T20:50:10.756112Z",
          "iopub.execute_input": "2024-10-28T20:50:10.757446Z",
          "iopub.status.idle": "2024-10-28T20:52:28.496577Z",
          "shell.execute_reply.started": "2024-10-28T20:50:10.757393Z",
          "shell.execute_reply": "2024-10-28T20:52:28.494959Z"
        },
        "trusted": true,
        "id": "qo-2mzuy74EI",
        "outputId": "73224f22-ba8f-44ef-9e5d-d8d90e837777"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Fitting 3 folds for each of 36 candidates, totalling 108 fits\nBest Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 300}\nBest Accuracy: 67.15%\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   2.7s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   5.6s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   7.8s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   6.8s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   2.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   6.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   2.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   4.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   7.7s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   4.6s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   6.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   6.9s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   2.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   5.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   7.0s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   6.5s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   2.4s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=   4.7s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   2.3s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   2.3s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   5.3s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=   6.9s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   4.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   5.7s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   7.8s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   4.7s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   6.8s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   6.6s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   6.8s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   2.7s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   4.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   6.8s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   6.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   2.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   4.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   7.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   7.1s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   6.6s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   7.0s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   2.3s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   5.3s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=   6.8s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=   5.9s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   2.7s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   5.7s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   2.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   3.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   4.7s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   6.6s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   4.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   7.9s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   6.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   2.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   2.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   2.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   4.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   6.9s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   5.0s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   6.4s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=   4.7s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   7.1s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=   7.0s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   4.3s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=   5.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   2.8s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   7.9s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   2.9s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   4.7s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   6.8s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   2.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   2.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   4.7s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   2.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   2.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   5.0s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   6.8s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   6.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   4.7s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   2.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   2.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   5.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   6.9s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   2.4s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   2.4s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=   4.7s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   7.0s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   5.0s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   4.3s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=   5.6s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Initialize individual classifiers\n",
        "clf1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf2 = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "clf3 = LogisticRegression(random_state=42)\n",
        "\n",
        "# Initialize the Voting Classifier\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('rf', clf1), ('xgb', clf2), ('lr', clf3)], voting='soft')\n",
        "\n",
        "# Train the classifier\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Voting Classifier Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-28T21:21:21.434391Z",
          "iopub.execute_input": "2024-10-28T21:21:21.434975Z",
          "iopub.status.idle": "2024-10-28T21:21:21.523854Z",
          "shell.execute_reply.started": "2024-10-28T21:21:21.434923Z",
          "shell.execute_reply": "2024-10-28T21:21:21.522014Z"
        },
        "trusted": true,
        "id": "bTPgwu7d74EJ",
        "outputId": "806b72b5-0c5a-48d4-ca86-76782a442651"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m clf1 \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      5\u001b[0m clf2 \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m clf3 \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegression\u001b[49m(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize the Voting Classifier\u001b[39;00m\n\u001b[1;32m      9\u001b[0m voting_clf \u001b[38;5;241m=\u001b[39m VotingClassifier(estimators\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     10\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, clf1), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb\u001b[39m\u001b[38;5;124m'\u001b[39m, clf2), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m, clf3)], voting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'LogisticRegression' is not defined",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "5. Feature Engineering\n",
        "Improving the quality of your features can significantly boost model performance. This includes creating new features, transforming existing ones, and selecting the most relevant features.\n",
        "\n",
        "6. Data Augmentation\n",
        "For image data, augmenting your dataset by applying transformations like rotations, flips, and color adjustments can help improve model generalization."
      ],
      "metadata": {
        "id": "Egnold7H74EJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}