{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrios80/GroceryStoreDataset/blob/master/code/taller2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TALLER 2\n",
        "\n",
        "### Objetivo:\n",
        "aplicar técnicas de machine learning, las cuales permitan descubrir insights, sugerir accionables al negocio y calcular el valor ganado.\n",
        "\n",
        "### Contexto del negocio\n",
        "Apoyo a un Supermercado Inteligente\n",
        "\n",
        "### Mision\n",
        "Mediante el uso de modelos de Machine Learning, en conjunto con técnicas de preparación de datos, se espera que usted esté en capacidad de construir el modelo que identifique los productos, y argumente el valor que generará al supermercado los resultados que obtenga.\n"
      ],
      "metadata": {
        "id": "65L8-jED_dhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Entendimiento y preparación de los datos:\n",
        "\n",
        "Reporte de entendimiento de datos.\n",
        "Dimensiones del dataset,\n",
        "Características de las imágenes\n",
        "Indicadores que considere importante.\n",
        "Integración de  técnicas de aumento de datos.\n",
        "Determine qué productos y que categorías empleará."
      ],
      "metadata": {
        "id": "qlRdH4BDAivd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "XbG3ycV774EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Entendimiento y preparacion de los datos.\n",
        "#Dataset:\n",
        "\n",
        "#Imagenes: 348x348 96dpi 24 bit depth"
      ],
      "metadata": {
        "id": "3LjNxjPz74EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install scikit-learn pandas numpy opencv-python\n",
        "\n",
        "\n",
        "#training\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data_dir = '/kaggle/input/apples/golden-delicious/'\n",
        "labels_file = os.path.join(data_dir, 'classes.csv')\n",
        "label_file_columns=[\"class_name\",\"class_id\",\"coarse_class_name\",\"coarse_class_id\",\"icon_path\",\"product_description_path\"]\n",
        "\n",
        "# Read the labels\n",
        "labels_df = pd.read_csv(labels_file)\n",
        "labels_df.columns=label_file_columns\n",
        "#print(labels_df)\n",
        "\n",
        "def  load_file(data_dir='', mode=\"train\"):\n",
        "    train_file=f\"{data_dir}{mode}.txt\"\n",
        "    dftrain_file=pd.read_csv(train_file)\n",
        "    train_file_columns=[\"filename\",\"class_id\", \"coarse_class_id\"]\n",
        "    dftrain_file.columns=train_file_columns\n",
        "    return dftrain_file\n",
        "#print(dftrain_file)\n",
        "\n",
        "dftrain_file=load_file(data_dir=data_dir, mode=\"train\")\n",
        "\n",
        "print('rows:' + str(dftrain_file.shape[0]))\n",
        "\n",
        "\n",
        "# Function to load images into an array\n",
        "def load_images(data_dir, df_files, image_size=348 ):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for index, row in df_files.iterrows():\n",
        "        img_path = os.path.join(data_dir, row['filename'])\n",
        "        #print(img_path)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.resize(img, (image_size, image_size))  # Resize to a fixed size\n",
        "        images.append(img)\n",
        "        labels.append(row['class_id'])\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "\n",
        "def preprocess_dataset(data_dir='', mode=\"train\" ):\n",
        "    dftrain_file = load_file(data_dir, mode)\n",
        "    images, labels = load_images(data_dir, dftrain_file )\n",
        "    # Normalize the images\n",
        "    # This line normalizes the pixel values of the images by dividing them by 255.0. This scales the pixel values from the original range (typically 0-255 for RGB images) to the range 0-1, which is a common normalization technique for neural networks.\n",
        "    # images = images / 255.0\n",
        "    # Flatten the images for the classifier\n",
        "    #This reshapes the images from their original shape\n",
        "    # (likely (n_samples, height, width, channels)) to a flat 2D array.\n",
        "    # The new shape is (n_samples, height * width * channels), where:\n",
        "    # height * width * channels represents the total number of pixels in each image.\n",
        "    # n_samples is the number of images in the dataset.\n",
        "    n_samples, height, width, channels = images.shape\n",
        "    print('before shaping:')\n",
        "    print(images.shape)\n",
        "    print( n_samples, height, width, channels )\n",
        "    images = images.reshape((n_samples, height * width * channels))\n",
        "    print('after reshaping:')\n",
        "    print(images.shape)\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "def preprocess_dataset_no_flattening(data_dir='', mode=\"train\" ):\n",
        "    dftrain_file = load_file(data_dir, mode)\n",
        "    images, labels = load_images(data_dir, dftrain_file, image_size=128 )\n",
        "    # Normalize the images\n",
        "    # This line normalizes the pixel values of the images by dividing them by 255.0. This scales the pixel values from the original range (typically 0-255 for RGB images) to the range 0-1, which is a common normalization technique for neural networks.\n",
        "    images = images / 255.0\n",
        "    print(images.shape)\n",
        "    return images, labels\n",
        "\n",
        "X_train, y_train = preprocess_dataset( data_dir,\"train\" )\n",
        "print(X_train)\n",
        "print(X_train.shape)\n",
        "print(y_train)\n",
        "\n",
        "X_test, y_test = preprocess_dataset( data_dir,\"test\" )\n",
        "\n",
        "\n",
        "# Encode the labels if necessary\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# le = LabelEncoder()\n",
        "# labels = le.fit_transform(labels)\n",
        "\n",
        "# Split the data\n",
        "#X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.0, random_state=42)\n",
        "#X_train =images\n",
        "#y_train =labels\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "###############\n",
        "\n",
        "# Initialize the classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-28T23:33:10.628042Z",
          "iopub.execute_input": "2024-10-28T23:33:10.628839Z",
          "iopub.status.idle": "2024-10-28T23:33:19.416887Z",
          "shell.execute_reply.started": "2024-10-28T23:33:10.628792Z",
          "shell.execute_reply": "2024-10-28T23:33:19.415631Z"
        },
        "trusted": true,
        "id": "c_ukxBIa74EF",
        "outputId": "4e74497e-50dc-498d-9ebb-4742f3239ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "rows:277\nbefore shaping:\n(277, 348, 348, 3)\n277 348 348 3\nafter reshaping:\n(277, 363312)\n[[ 37  20 147 ...  29 126 160]\n [ 13  61  89 ...  79 107 142]\n [ 26 144 193 ...  37  82 115]\n ...\n [ 22  33  47 ...  43  68 102]\n [ 43  65 171 ... 104 181 243]\n [  0 114 117 ...  40  35 144]]\n(277, 363312)\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4\n 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\nbefore shaping:\n(275, 348, 348, 3)\n275 348 348 3\nafter reshaping:\n(275, 363312)\n(277, 363312)\nAccuracy: 59.27%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the classifier\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'XGBoost Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-28T23:23:12.572640Z",
          "iopub.execute_input": "2024-10-28T23:23:12.573092Z",
          "iopub.status.idle": "2024-10-28T23:28:14.238742Z",
          "shell.execute_reply.started": "2024-10-28T23:23:12.573048Z",
          "shell.execute_reply": "2024-10-28T23:28:14.237191Z"
        },
        "trusted": true,
        "id": "c02bLRql74EH",
        "outputId": "357a6b90-a186-4e7e-d05c-b5a4633ae463"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "\nKeyboardInterrupt\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "\n",
        "\n",
        "X_train, y_train = preprocess_dataset_no_flattening( data_dir,\"train\" )\n",
        "X_test, y_test = preprocess_dataset_no_flattening( data_dir,\"test\" )\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')  # Assuming 10 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'CNN Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-28T23:33:24.709776Z",
          "iopub.execute_input": "2024-10-28T23:33:24.710655Z",
          "iopub.status.idle": "2024-10-28T23:34:13.055423Z",
          "shell.execute_reply.started": "2024-10-28T23:33:24.710605Z",
          "shell.execute_reply": "2024-10-28T23:34:13.054214Z"
        },
        "trusted": true,
        "id": "Io8MqXDp74EI",
        "outputId": "c9391b28-545b-4b39-b03c-678f14f01656"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "(277, 128, 128, 3)\n(275, 128, 128, 3)\nEpoch 1/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 523ms/step - accuracy: 0.2620 - loss: 4.0262 - val_accuracy: 0.4436 - val_loss: 1.1451\nEpoch 2/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 471ms/step - accuracy: 0.5777 - loss: 0.9984 - val_accuracy: 0.5964 - val_loss: 0.8401\nEpoch 3/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 471ms/step - accuracy: 0.7931 - loss: 0.5782 - val_accuracy: 0.5309 - val_loss: 0.8602\nEpoch 4/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 455ms/step - accuracy: 0.9280 - loss: 0.3139 - val_accuracy: 0.6727 - val_loss: 0.6822\nEpoch 5/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 468ms/step - accuracy: 0.9908 - loss: 0.1264 - val_accuracy: 0.7273 - val_loss: 0.6469\nEpoch 6/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 466ms/step - accuracy: 1.0000 - loss: 0.0425 - val_accuracy: 0.6691 - val_loss: 0.7850\nEpoch 7/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 445ms/step - accuracy: 1.0000 - loss: 0.0164 - val_accuracy: 0.6509 - val_loss: 0.8393\nEpoch 8/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 453ms/step - accuracy: 1.0000 - loss: 0.0082 - val_accuracy: 0.6582 - val_loss: 0.8317\nEpoch 9/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 477ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.6873 - val_loss: 0.8328\nEpoch 10/10\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 464ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.6618 - val_loss: 0.8208\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.6684 - loss: 0.9236\nCNN Accuracy: 66.18%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize the classifier\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and accuracy\n",
        "best_params = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "print(f'Best Parameters: {best_params}')\n",
        "print(f'Best Accuracy: {best_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-28T20:50:10.756112Z",
          "iopub.execute_input": "2024-10-28T20:50:10.757446Z",
          "iopub.status.idle": "2024-10-28T20:52:28.496577Z",
          "shell.execute_reply.started": "2024-10-28T20:50:10.757393Z",
          "shell.execute_reply": "2024-10-28T20:52:28.494959Z"
        },
        "trusted": true,
        "id": "qo-2mzuy74EI",
        "outputId": "73224f22-ba8f-44ef-9e5d-d8d90e837777"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Fitting 3 folds for each of 36 candidates, totalling 108 fits\nBest Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 300}\nBest Accuracy: 67.15%\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   2.7s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   5.6s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   7.8s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   6.8s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   2.3s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   6.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   2.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   4.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   7.7s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   4.6s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   6.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   6.9s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   2.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   5.4s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   7.0s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   6.5s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   2.4s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=   4.7s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   2.3s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   2.3s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   5.3s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=   6.9s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   4.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   5.7s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   7.8s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   4.7s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   6.8s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   6.6s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   6.8s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   2.7s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   4.5s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   6.8s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   6.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   2.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   4.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   7.2s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   7.1s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   6.6s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   7.0s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   2.3s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   5.3s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=   6.8s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=   5.9s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   2.7s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   5.7s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   2.4s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   3.2s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   4.7s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   6.6s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   4.7s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   7.9s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   6.7s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   2.3s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   2.3s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   2.4s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   4.6s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   6.9s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   5.0s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   6.4s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=   4.7s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   7.1s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=   7.0s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   4.3s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=   5.3s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   2.8s\n[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   7.9s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   2.9s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   4.7s\n[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   6.8s\n[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   2.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   2.4s\n[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   4.7s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   2.3s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   2.2s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   5.0s\n[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   6.8s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   6.5s\n[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   4.7s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   2.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   2.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   5.3s\n[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   6.9s\n[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   4.4s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   2.4s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   2.4s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=   4.7s\n[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   7.0s\n[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   5.0s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   2.2s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   4.3s\n[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=   5.6s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Initialize individual classifiers\n",
        "clf1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf2 = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "clf3 = LogisticRegression(random_state=42)\n",
        "\n",
        "# Initialize the Voting Classifier\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('rf', clf1), ('xgb', clf2), ('lr', clf3)], voting='soft')\n",
        "\n",
        "# Train the classifier\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Voting Classifier Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-28T21:21:21.434391Z",
          "iopub.execute_input": "2024-10-28T21:21:21.434975Z",
          "iopub.status.idle": "2024-10-28T21:21:21.523854Z",
          "shell.execute_reply.started": "2024-10-28T21:21:21.434923Z",
          "shell.execute_reply": "2024-10-28T21:21:21.522014Z"
        },
        "trusted": true,
        "id": "bTPgwu7d74EJ",
        "outputId": "806b72b5-0c5a-48d4-ca86-76782a442651"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m clf1 \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      5\u001b[0m clf2 \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m clf3 \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegression\u001b[49m(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize the Voting Classifier\u001b[39;00m\n\u001b[1;32m      9\u001b[0m voting_clf \u001b[38;5;241m=\u001b[39m VotingClassifier(estimators\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     10\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, clf1), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb\u001b[39m\u001b[38;5;124m'\u001b[39m, clf2), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m, clf3)], voting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'LogisticRegression' is not defined",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "5. Feature Engineering\n",
        "Improving the quality of your features can significantly boost model performance. This includes creating new features, transforming existing ones, and selecting the most relevant features.\n",
        "\n",
        "6. Data Augmentation\n",
        "For image data, augmenting your dataset by applying transformations like rotations, flips, and color adjustments can help improve model generalization."
      ],
      "metadata": {
        "id": "Egnold7H74EJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}